{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: Compare and contrast NLTK and spaCy in terms of features, ease of use,\n",
        "and performance.**\n",
        "-**Features:** NLTK is feature-rich and geared toward research and learning.offering tokenization,POS tagging,stemming,lemmatization,parsing,and access to extensive corpora like WordNet. spaCy focuses on production-ready NLP with fast,accurate pretrained models for tokenization,POS tagging,dependency parsing,and named entity recognition,though it has fewer built-in linguistic resources.\n",
        "\n",
        "**Ease of Use:** NLTK has a steeper learning curve because its modular design often requires manually combining steps.spaCy provides clear, developer-friendly APIs with end-to-end pipelines,allowing tasks like tokenization,tagging,parsing.\n",
        "\n",
        "**Performance:** NLTK is slower and less memory-efficient,making it better suited for small-scale tasks or experimentation.spaCy is optimized with Cython for speed,high accuracy,and scalability,handling large datasets efficiently and making it ideal for production environments.\n"
      ],
      "metadata": {
        "id": "vmOPEVP5_NII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is TextBlob and how does it simplify common NLP tasks like\n",
        "sentiment analysis and translation?**\n",
        "- TextBlob is a high-level Python library built on top of NLTK and Pattern that simplifies many common natural language processing (NLP) tasks.It provides an easy-to-use API for processing textual data without requiring deep knowledge of NLP algorithms or building complex pipelines.\n",
        "\n",
        "- For sentiment analysis, TextBlob allows you to determine the polarity (positive or negative sentiment) and subjectivity of a text with just a few lines of code. For example, TextBlob(\"I love this!\").sentiment returns the sentiment score directly, eliminating the need to manually tokenize,clean,or train a model.\n",
        "\n",
        "- For translation and language detection, TextBlob leverages external services like Google Translate."
      ],
      "metadata": {
        "id": "WhPwi-YT_8qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: Explain the role of Standford NLP in academic and industry NLP Projects.**\n",
        "\n",
        "- Stanford NLP is a suite of powerful natural language processing tools developed by the Stanford NLP Group.It provides state-of-the-art algorithms for tasks such as tokenization,part-of-speech tagging,named entity recognition (NER),dependency parsing,coreference resolution,and sentiment analysis.\n",
        "\n",
        "- **Academic projects**-Stanford NLP is widely used for research and experimentation because of its strong theoretical foundations,high accuracy, and support for multiple languages.Researchers often use it to benchmark new NLP models, study linguistic phenomena or preprocess text data for machine learning experiments.\n",
        "\n",
        "- **Industry applications**-Stanford NLP helps build production-ready systems that require accurate language understanding,such as chatbots,information extraction systems,document summarization,and question-answering engines.Its pre-trained models allow developers to implement advanced NLP functionalities without building models from scratch."
      ],
      "metadata": {
        "id": "i30-Ymz8AIWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Describe the architecture and functioning of a Recurrent Natural Network\n",
        "(RNN).**\n",
        "- A Recurrent Neural Network (RNN) is a type of neural network designed to handle sequential data,such as text,time series speech.Unlike traditional feedforward networks,RNNs have loops in their architecture that allow information to persist across time steps,enabling the network to retain context from previous inputs.\n",
        "- RNNs process sequences step by step,updating hidden states and generating outputs at each time step.They use backpropagation through time (BPTT) to adjust weights based on sequence errors.This makes them suitable for tasks like language modeling,sentiment analysis,machine translation,and speech recognition.\n",
        "- Architecture of RNN consists of an input layer,a hidden layer with recurrent connections, and an output layer."
      ],
      "metadata": {
        "id": "FACUc0niANHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What is the key difference between LSTM and GRU networks in NLP\n",
        "applications?**\n",
        "- The key difference between LSTM and GRU networks in NLP is that LSTMs have three gates(input, forget, output) and a separate memory cell to handle long-term dependencies,while GRUs have only two gates(update and reset)and combine memory functions,making them simpler,faster,and more efficient.\n"
      ],
      "metadata": {
        "id": "NYlrmcTXAQxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program using TextBlob to perform sentiment analysis on\n",
        "the following paragraph of text:\n",
        "“I had a great experience using the new mobile banking app. The interface is intuitive,\n",
        "and customer support was quick to resolve my issue. However, the app did crash once\n",
        "during a transaction, which was frustrating\"\n",
        "Your program should print out the polarity and subjectivity scores.**"
      ],
      "metadata": {
        "id": "jijVyNDOAaaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "text = \"\"\"I had a great experience using the new mobile banking app.\n",
        "The interface is intuitive, and customer support was quick to resolve my issue.\n",
        "However, the app did crash once during a transaction, which was frustrating.\"\"\"\n",
        "blob = TextBlob(text)\n",
        "polarity = blob.sentiment.polarity\n",
        "subjectivity = blob.sentiment.subjectivity\n",
        "# Print results\n",
        "print(f\"Polarity: {polarity}\")\n",
        "print(f\"Subjectivity: {subjectivity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGOA7SdMEu7p",
        "outputId": "7202b08a-2b1f-4c9a-8693-e5d903c8bc87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.21742424242424244\n",
            "Subjectivity: 0.6511363636363636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”**"
      ],
      "metadata": {
        "id": "eICNr_KcAiuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science,and artificial intelligence.It enables machines to understand,\n",
        "interpret,and generate human language.Applications of NLP include chatbots,\n",
        "sentiment analysis,and machine translation.As technology advances,the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "tokens = word_tokenize(text)\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(\"Tokens:\\n\", tokens, \"\\n\")\n",
        "\n",
        "\n",
        "print(\"Frequency Distribution:\")\n",
        "for word, frequency in freq_dist.items():\n",
        "    print(f\"{word}: {frequency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "C-le4Sk0FMAZ",
        "outputId": "bc7cdc5b-9c8a-4831-8ded-460a7af64187"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-551527390.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0mmachine\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAs\u001b[0m \u001b[0mtechnology\u001b[0m \u001b[0madvances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthe\u001b[0m \u001b[0mrole\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m in modern solutions is becoming increasingly critical.\"\"\"\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mfreq_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Implement a basic LSTM model in Keras for a text classification task using\n",
        "the following dummy dataset. Your model should classify sentences as either positive\n",
        "(1) or negative (0).**\n",
        "# Dataset\n",
        "**texts = [\n",
        "“I love this project”, #Positive\n",
        "“This is an amazing experience”, #Positive\n",
        "“I hate waiting in line”, #Negative\n",
        "“This is the worst service”, #Negative\n",
        "“Absolutely fantastic!” #Positive\n",
        "]\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on\n",
        "this data. You may use Keras with TensorFlow backend.**"
      ],
      "metadata": {
        "id": "dXpaD5HkAod4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "texts = [\n",
        "    \"I love this project\",\n",
        "    \"This is an amazing experience\",\n",
        "    \"I hate waiting in line\",\n",
        "    \"This is the worst service\",\n",
        "    \"Absolutely fantastic!\"\n",
        "]\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 10\n",
        "embedding_dim = 50\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "labels = np.array(labels)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(padded_sequences, labels, epochs=20, verbose=1)\n",
        "\n",
        "test_text = [\"I really enjoyed this service\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "prediction = model.predict(test_pad)\n",
        "print(f\"Predicted sentiment (0=Negative, 1=Positive): {prediction[0][0]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-3GUni5Fpj0",
        "outputId": "60f802ac-a71f-4df0-bee7-1b9c4011fd5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.4000 - loss: 0.6937\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6000 - loss: 0.6901\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6867\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6834\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6000 - loss: 0.6800\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.6000 - loss: 0.6766\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 0.6731\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6694\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6000 - loss: 0.6655\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6614\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6570\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6000 - loss: 0.6523\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6472\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 0.6417\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6356\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6000 - loss: 0.6285\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6000 - loss: 0.6202\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6000 - loss: 0.6102\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 0.5979\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6000 - loss: 0.5829\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
            "Predicted sentiment (0=Negative, 1=Positive): 0.6703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Using spaCy, build a simple NLP pipeline that includes tokenization,\n",
        "lemmatization, and entity recognition. Use the following paragraph as your dataset:\n",
        "“Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "development of India’s atomic energy program. He was the founding director of the Tata\n",
        "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "Atomic Energy Commission of India.”\n",
        "Write a Python program that processes this text using spaCy, then prints tokens, their\n",
        "lemmas, and any named entities found.**"
      ],
      "metadata": {
        "id": "bjdyjpJpBFge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "text = \"\"\"Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "development of India’s atomic energy program. He was the founding director of the Tata\n",
        "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "Atomic Energy Commission of India.\"\"\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Tokens and Lemmas:\\n\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.lemma_}\")\n",
        "\n",
        "print(\"\\nNamed Entities:\\n\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP3wMw7NFw2c",
        "outputId": "eaaec0ae-05a2-4a39-965e-fd72084bc296"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens and Lemmas:\n",
            "\n",
            "Homi -> Homi\n",
            "Jehangir -> Jehangir\n",
            "Bhaba -> Bhaba\n",
            "was -> be\n",
            "an -> an\n",
            "Indian -> indian\n",
            "nuclear -> nuclear\n",
            "physicist -> physicist\n",
            "who -> who\n",
            "played -> play\n",
            "a -> a\n",
            "key -> key\n",
            "role -> role\n",
            "in -> in\n",
            "the -> the\n",
            "\n",
            " -> \n",
            "\n",
            "development -> development\n",
            "of -> of\n",
            "India -> India\n",
            "’s -> ’s\n",
            "atomic -> atomic\n",
            "energy -> energy\n",
            "program -> program\n",
            ". -> .\n",
            "He -> he\n",
            "was -> be\n",
            "the -> the\n",
            "founding -> found\n",
            "director -> director\n",
            "of -> of\n",
            "the -> the\n",
            "Tata -> Tata\n",
            "\n",
            " -> \n",
            "\n",
            "Institute -> Institute\n",
            "of -> of\n",
            "Fundamental -> Fundamental\n",
            "Research -> Research\n",
            "( -> (\n",
            "TIFR -> TIFR\n",
            ") -> )\n",
            "and -> and\n",
            "was -> be\n",
            "instrumental -> instrumental\n",
            "in -> in\n",
            "establishing -> establish\n",
            "the -> the\n",
            "\n",
            " -> \n",
            "\n",
            "Atomic -> Atomic\n",
            "Energy -> Energy\n",
            "Commission -> Commission\n",
            "of -> of\n",
            "India -> India\n",
            ". -> .\n",
            "\n",
            "Named Entities:\n",
            "\n",
            "Homi Jehangir Bhaba (FAC)\n",
            "Indian (NORP)\n",
            "India (GPE)\n",
            "the Tata \n",
            "Institute of Fundamental Research (ORG)\n",
            "Atomic Energy Commission of India (ORG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working on a chatbot for a mental health platform. Explain how\n",
        "you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford\n",
        "NLP to understand and respond to user input effectively. Detail your architecture, data\n",
        "preprocessing pipeline, and any ethical considerations.**\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "1. **Input Layer:** Accepts user messages as text.\n",
        "2. **Preprocessing Layer:** Uses spaCy or Stanford NLP to clean and process text. Tasks include tokenization, lemmatization, part-of-speech tagging, and named entity recognition to identify relevant entities (like symptoms or emotions).\n",
        "3. **Embedding Layer:** Converts words into dense vectors using embeddings such as Word2Vec, GloVe, or spaCy’s built-in vectors to capture semantic meaning.\n",
        "4. **Sequence Model:** An LSTM or GRU layer processes the sequence of embeddings, capturing temporal dependencies and context across sentences.GRUs may be preferred for faster training and fewer parameters, while LSTMs are better at handling longer contexts.\n",
        "5. **Output Layer:** A dense layer with softmax (for multi-class intent classification) or sigmoid (for binary sentiment/emotion detection) predicts the user’s intent or emotional state.\n",
        "6. **Response Generation:** Based on the predicted intent or sentiment, a response is generated either via predefined templates or using a generative model.\n",
        "\n",
        "**Data Preprocessing Pipeline:**\n",
        "\n",
        "* **Text cleaning:** Remove punctuation, lowercase conversion, handle contractions.\n",
        "* **Tokenization & Lemmatization:** spaCy/Stanford NLP ensures that words are in canonical form.\n",
        "* **Stopword removal & entity extraction:** Optional, depending on the importance of stopwords in context.\n",
        "* **Sequence padding:** Ensures uniform input length for the LSTM/GRU.\n",
        "\n",
        "**Ethical Considerations:**\n",
        "\n",
        "* Ensure user privacy and data security for sensitive mental health information.\n",
        "* Implement safeguards for crisis situations,such as triggering human intervention when high-risk keywords are detected.\n",
        "* Avoid giving definitive medical advice; responses should be supportive, empathetic, and direct users to professional help if needed.\n",
        "* Monitor bias in training data to avoid harmful or insensitive responses.\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion**-spaCy or Stanford NLP handles robust text preprocessing and entity recognition,while LSTM or GRU networks model sequential patterns and context to classify intent or sentiment.Combining these ensures the chatbot understands user inputs effectively and responds in a safe,empathetic,and context-aware manner.\n"
      ],
      "metadata": {
        "id": "HWvGV6A-BCbP"
      }
    }
  ]
}